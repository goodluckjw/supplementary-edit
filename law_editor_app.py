import streamlit as st
import pandas as pd
import re
from collections import defaultdict
import io
import requests
from bs4 import BeautifulSoup
import time

def extract_article(text):
    match = re.search(r"(ì œ\d+ì¡°(?:ì˜\d+)?)", text)
    return match.group(1) if match else ""

def extract_title(text):
    match = re.search(r"ì œ\d+ì¡°(?:ì˜\d+)?\(([^)]+)\)", text)
    return match.group(1) if match else None

def extract_clause_number(text):
    match = re.search(r"([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³])", text)
    circled_number_map = {
        "â‘ ": "1", "â‘¡": "2", "â‘¢": "3", "â‘£": "4", "â‘¤": "5",
        "â‘¥": "6", "â‘¦": "7", "â‘§": "8", "â‘¨": "9", "â‘©": "10",
        "â‘ª": "11", "â‘«": "12", "â‘¬": "13", "â‘­": "14", "â‘®": "15",
        "â‘¯": "16", "â‘°": "17", "â‘±": "18", "â‘²": "19", "â‘³": "20",
    }
    return circled_number_map.get(match.group(1), None) if match else None

def has_final_consonant(korean_word):
    last_char = korean_word[-1]
    return (ord(last_char) - 44032) % 28 != 0

def format_clauses(clauses):
    if len(clauses) == 1:
        return f"ì œ{clauses[0]}í•­"
    elif len(clauses) == 2:
        return f"ì œ{clauses[0]}í•­ ë° ì œ{clauses[1]}í•­"
    else:
        formatted = ", ".join([f"ì œ{clause}í•­" for clause in clauses[:-1]])
        return f"{formatted} ë° ì œ{clauses[-1]}í•­"

def number_to_circled(num):
    if 1 <= num <= 50:
        return chr(0x2460 + num - 1)
    elif 51 <= num <= 100:
        return chr(0x3251 + num - 51)
    else:
        return f"({num})"

def fetch_law_names_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    law_names = []
    for td in soup.find_all('td', class_='tl'):
        law_name = td.get_text(strip=True)
        if law_name:
            law_names.append(law_name)
    return law_names

def fetch_law_text_from_law_go_kr(law_name):
    search_url = f"https://www.law.go.kr/LSW/eng/engLsSc.do?menuId=2&query={law_name}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    law_text = ""
    for div in soup.find_all("div", class_="law_text"):
        law_text += div.get_text(separator="\n", strip=True)
    return law_text if law_text else law_name + " ì „ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def process_web_laws(url, original_term, replacement_term):
    law_names = fetch_law_names_from_url(url)
    output_lines = []

    for idx, law_name in enumerate(sorted(law_names), 1):
        time.sleep(0.5)  # avoid hammering site
        text = fetch_law_text_from_law_go_kr(law_name)
        if original_term not in text:
            continue
        circled = number_to_circled(idx)
        article = extract_article(text) or "ì œ1ì¡°"
        clause_number = extract_clause_number(text)
        title = extract_title(text)
        clauses = [clause_number] if clause_number else []

        clause_text = ""
        if title and clauses:
            clause_text = f"{article}ì˜ ì œëª© ë° ê°™ì€ ì¡° {format_clauses(clauses)}"
        elif title:
            clause_text = f"{article}ì˜ ì œëª©"
        elif clauses:
            clause_text = f"{article} {format_clauses(clauses)}"
        else:
            clause_text = f"{article}"

        matches = re.findall(rf"[ê°€-í£]*{original_term}(?=\(|\s|\.|,|$)?", text)
        for match in sorted(set(matches)):
            modified = match.replace(original_term, replacement_term, 1)
            particle = "ì„" if has_final_consonant(match) else "ë¥¼"
            count = len(re.findall(re.escape(match), text))
            each = "ê°ê° " if count > 1 else ""
            sentence = f'{law_name} {clause_text} ì¤‘ "{match}"{particle} {each}"{modified}"ìœ¼ë¡œ í•œë‹¤.'
            output_lines.append(f"{circled} {law_name} ì¼ë¶€ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°œì •í•œë‹¤.")
            output_lines.append(sentence)
            output_lines.append("")

    return "\n".join(output_lines)

st.title("íƒ€ë²•ê°œì • ë„ìš°ë¯¸")

st.markdown("""
**ìš©ì–´ ê°œì • í›„ ë¶€ì¹™ì˜ 'ë‹¤ë¥¸ ë²•ë¥ ì˜ ê°œì •'ì„ ê°„í¸í•˜ê²Œ í•˜ê¸° ìœ„í•œ ì•±ì…ë‹ˆë‹¤.**

**ê°œì„ í•  ì‚¬í•­ì´ë‚˜ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì‚¬ë²•ë²•ì œê³¼ ê¹€ì¬ìš°(4778)ë¡œ ì—°ë½ì£¼ì„¸ìš”.**

**ì‚¬ìš© ë°©ë²•:**
1. êµ­íšŒ ì…ë²•ì •ë³´ì„¼í„°ì˜ ë²•ë¥  ê²€ìƒ‰ URLì„ ì…ë ¥í•˜ì„¸ìš”.  
2. ë°”ê¾¸ê³  ì‹¶ì€ ë‹¨ì–´ì™€ ìƒˆë¡œ ë°”ê¿€ ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.  
3. í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•´ì„œ ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆì–´ìš”.
""")

url = st.text_input("ë²•ë¥  ëª©ë¡ì´ ìˆëŠ” URL ì…ë ¥")
original_term = st.text_input("ì°¾ì„ ë‹¨ì–´ (ì˜ˆ: ì§€ë°©ë²•ì›)")
replacement_term = st.text_input("ë°”ê¿€ ë‹¨ì–´ (ì˜ˆ: ì§€ì—­ë²•ì›)")

if url and original_term and replacement_term:
    result_text = process_web_laws(url, original_term, replacement_term)

    st.download_button(
        label="ğŸ“¥ ê²°ê³¼ í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=result_text,
        file_name="ë²•ë¥ _ê°œì •ì•ˆ_ê²°ê³¼.txt",
        mime="text/plain"
    )

    st.text_area("ë¯¸ë¦¬ë³´ê¸°", result_text, height=400)
else:
    st.info("ë²•ë¥  URLê³¼ ë°”ê¿€ ë‹¨ì–´ë“¤ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
